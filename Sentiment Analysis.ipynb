{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Sentiment Analysis [40 points]\n",
    "\n",
    "Download data from: http://www.andrew.cmu.edu/user/georgech/HW3-data.zip <br>\n",
    "\n",
    "The folder contains:\n",
    "\n",
    "- `train.csv`\n",
    "- `test.csv`\n",
    "\n",
    "In this problem, we look at predicting whether a tweet has positive or negative sentiment. We intentionally write this problem to be a bit open-ended to let you play with different neural net code yourself and also for you to compare against a classical baseline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(a) [5 points]** We first read in the tweet data. Read the data present in `train.csv` file. **Please do not change the filename. In particular, use a relative path, i.e., `./HW3-data/train.csv`**. Then do the following:\n",
    "\n",
    "1. Keep only the sentiment and sentiment text in the data - the first and the last coumn\n",
    "2. Print the number of positive and negative sentiment labels\n",
    "\n",
    "Note: If you are using `open()`, you may have to set `encoding='iso8859'`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of positive labels: 800000\n",
      "Number of negative labels: 800000\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import csv\n",
    "def load_csv(csv_filename):\n",
    "    labels = []\n",
    "    tweets = []\n",
    "    with open(csv_filename, encoding='iso8859') as f:\n",
    "        csv_reader = csv.reader(f)\n",
    "        for row in csv_reader:\n",
    "            labels.append(int(row[0]))\n",
    "            tweets.append(row[-1])\n",
    "    return tweets, np.array(labels)\n",
    "\n",
    "train_text, train_labels = load_csv('HW3-data/train.csv')\n",
    "print('Number of positive labels:', (train_labels == 1).sum())\n",
    "print('Number of negative labels:', (train_labels == 0).sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(b) [25 points]** Modify the LSTM demo code from lecture to work with this Twitter dataset. Play with the learning rate and batch size so that the training gives reasonable increases in training and validation accuracies as we progress through epochs (leave the number of epochs at 10). Also feel free to try different neural net architectures (although you do not have to). What test accuracy are you able to achieve?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "train_dataset = list(zip(train_text, train_labels))\n",
    "\n",
    "proper_train_size = int(len(train_dataset) * 0.8)\n",
    "val_size = len(train_dataset) - proper_train_size\n",
    "\n",
    "torch.manual_seed(0)\n",
    "proper_train_dataset, val_dataset = torch.utils.data.random_split(train_dataset,\n",
    "                                                                  [proper_train_size,\n",
    "                                                                   val_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tweet 10000/1280000, dictionary size: 18070\n",
      "Parsing tweet 20000/1280000, dictionary size: 30016\n",
      "Parsing tweet 30000/1280000, dictionary size: 40690\n",
      "Parsing tweet 40000/1280000, dictionary size: 50442\n",
      "Parsing tweet 50000/1280000, dictionary size: 59643\n",
      "Parsing tweet 60000/1280000, dictionary size: 68236\n",
      "Parsing tweet 70000/1280000, dictionary size: 76487\n",
      "Parsing tweet 80000/1280000, dictionary size: 84484\n",
      "Parsing tweet 90000/1280000, dictionary size: 92350\n",
      "Parsing tweet 100000/1280000, dictionary size: 100059\n",
      "Parsing tweet 110000/1280000, dictionary size: 107685\n",
      "Parsing tweet 120000/1280000, dictionary size: 115073\n",
      "Parsing tweet 130000/1280000, dictionary size: 122191\n",
      "Parsing tweet 140000/1280000, dictionary size: 129093\n",
      "Parsing tweet 150000/1280000, dictionary size: 135849\n",
      "Parsing tweet 160000/1280000, dictionary size: 142701\n",
      "Parsing tweet 170000/1280000, dictionary size: 149362\n",
      "Parsing tweet 180000/1280000, dictionary size: 155947\n",
      "Parsing tweet 190000/1280000, dictionary size: 162456\n",
      "Parsing tweet 200000/1280000, dictionary size: 168714\n",
      "Parsing tweet 210000/1280000, dictionary size: 174981\n",
      "Parsing tweet 220000/1280000, dictionary size: 181309\n",
      "Parsing tweet 230000/1280000, dictionary size: 187617\n",
      "Parsing tweet 240000/1280000, dictionary size: 193768\n",
      "Parsing tweet 250000/1280000, dictionary size: 199612\n",
      "Parsing tweet 260000/1280000, dictionary size: 205414\n",
      "Parsing tweet 270000/1280000, dictionary size: 211103\n",
      "Parsing tweet 280000/1280000, dictionary size: 216802\n",
      "Parsing tweet 290000/1280000, dictionary size: 222524\n",
      "Parsing tweet 300000/1280000, dictionary size: 228137\n",
      "Parsing tweet 310000/1280000, dictionary size: 233733\n",
      "Parsing tweet 320000/1280000, dictionary size: 239319\n",
      "Parsing tweet 330000/1280000, dictionary size: 245061\n",
      "Parsing tweet 340000/1280000, dictionary size: 250576\n",
      "Parsing tweet 350000/1280000, dictionary size: 256005\n",
      "Parsing tweet 360000/1280000, dictionary size: 261525\n",
      "Parsing tweet 370000/1280000, dictionary size: 266901\n",
      "Parsing tweet 380000/1280000, dictionary size: 272094\n",
      "Parsing tweet 390000/1280000, dictionary size: 277303\n",
      "Parsing tweet 400000/1280000, dictionary size: 282688\n",
      "Parsing tweet 410000/1280000, dictionary size: 287940\n",
      "Parsing tweet 420000/1280000, dictionary size: 293141\n",
      "Parsing tweet 430000/1280000, dictionary size: 298165\n",
      "Parsing tweet 440000/1280000, dictionary size: 303292\n",
      "Parsing tweet 450000/1280000, dictionary size: 308272\n",
      "Parsing tweet 460000/1280000, dictionary size: 313349\n",
      "Parsing tweet 470000/1280000, dictionary size: 318209\n",
      "Parsing tweet 480000/1280000, dictionary size: 323256\n",
      "Parsing tweet 490000/1280000, dictionary size: 328141\n",
      "Parsing tweet 500000/1280000, dictionary size: 332920\n",
      "Parsing tweet 510000/1280000, dictionary size: 337795\n",
      "Parsing tweet 520000/1280000, dictionary size: 342756\n",
      "Parsing tweet 530000/1280000, dictionary size: 347616\n",
      "Parsing tweet 540000/1280000, dictionary size: 352376\n",
      "Parsing tweet 550000/1280000, dictionary size: 357096\n",
      "Parsing tweet 560000/1280000, dictionary size: 361876\n",
      "Parsing tweet 570000/1280000, dictionary size: 366579\n",
      "Parsing tweet 580000/1280000, dictionary size: 371297\n",
      "Parsing tweet 590000/1280000, dictionary size: 375960\n",
      "Parsing tweet 600000/1280000, dictionary size: 380557\n",
      "Parsing tweet 610000/1280000, dictionary size: 385200\n",
      "Parsing tweet 620000/1280000, dictionary size: 389773\n",
      "Parsing tweet 630000/1280000, dictionary size: 394294\n",
      "Parsing tweet 640000/1280000, dictionary size: 398773\n",
      "Parsing tweet 650000/1280000, dictionary size: 403429\n",
      "Parsing tweet 660000/1280000, dictionary size: 407897\n",
      "Parsing tweet 670000/1280000, dictionary size: 412383\n",
      "Parsing tweet 680000/1280000, dictionary size: 416808\n",
      "Parsing tweet 690000/1280000, dictionary size: 421138\n",
      "Parsing tweet 700000/1280000, dictionary size: 425633\n",
      "Parsing tweet 710000/1280000, dictionary size: 429983\n",
      "Parsing tweet 720000/1280000, dictionary size: 434300\n",
      "Parsing tweet 730000/1280000, dictionary size: 438637\n",
      "Parsing tweet 740000/1280000, dictionary size: 443048\n",
      "Parsing tweet 750000/1280000, dictionary size: 447367\n",
      "Parsing tweet 760000/1280000, dictionary size: 451605\n",
      "Parsing tweet 770000/1280000, dictionary size: 455824\n",
      "Parsing tweet 780000/1280000, dictionary size: 459945\n",
      "Parsing tweet 790000/1280000, dictionary size: 464160\n",
      "Parsing tweet 800000/1280000, dictionary size: 468406\n",
      "Parsing tweet 810000/1280000, dictionary size: 472692\n",
      "Parsing tweet 820000/1280000, dictionary size: 476895\n",
      "Parsing tweet 830000/1280000, dictionary size: 481140\n",
      "Parsing tweet 840000/1280000, dictionary size: 485274\n",
      "Parsing tweet 850000/1280000, dictionary size: 489411\n",
      "Parsing tweet 860000/1280000, dictionary size: 493516\n",
      "Parsing tweet 870000/1280000, dictionary size: 497558\n",
      "Parsing tweet 880000/1280000, dictionary size: 501707\n",
      "Parsing tweet 890000/1280000, dictionary size: 505825\n",
      "Parsing tweet 900000/1280000, dictionary size: 509693\n",
      "Parsing tweet 910000/1280000, dictionary size: 513689\n",
      "Parsing tweet 920000/1280000, dictionary size: 517649\n",
      "Parsing tweet 930000/1280000, dictionary size: 521615\n",
      "Parsing tweet 940000/1280000, dictionary size: 525562\n",
      "Parsing tweet 950000/1280000, dictionary size: 529621\n",
      "Parsing tweet 960000/1280000, dictionary size: 533502\n",
      "Parsing tweet 970000/1280000, dictionary size: 537391\n",
      "Parsing tweet 980000/1280000, dictionary size: 541396\n",
      "Parsing tweet 990000/1280000, dictionary size: 545335\n",
      "Parsing tweet 1000000/1280000, dictionary size: 549170\n",
      "Parsing tweet 1010000/1280000, dictionary size: 552978\n",
      "Parsing tweet 1020000/1280000, dictionary size: 556844\n",
      "Parsing tweet 1030000/1280000, dictionary size: 560732\n",
      "Parsing tweet 1040000/1280000, dictionary size: 564583\n",
      "Parsing tweet 1050000/1280000, dictionary size: 568375\n",
      "Parsing tweet 1060000/1280000, dictionary size: 572208\n",
      "Parsing tweet 1070000/1280000, dictionary size: 575909\n",
      "Parsing tweet 1080000/1280000, dictionary size: 579648\n",
      "Parsing tweet 1090000/1280000, dictionary size: 583564\n",
      "Parsing tweet 1100000/1280000, dictionary size: 587431\n",
      "Parsing tweet 1110000/1280000, dictionary size: 591167\n",
      "Parsing tweet 1120000/1280000, dictionary size: 594805\n",
      "Parsing tweet 1130000/1280000, dictionary size: 598532\n",
      "Parsing tweet 1140000/1280000, dictionary size: 602282\n",
      "Parsing tweet 1150000/1280000, dictionary size: 605939\n",
      "Parsing tweet 1160000/1280000, dictionary size: 609712\n",
      "Parsing tweet 1170000/1280000, dictionary size: 613317\n",
      "Parsing tweet 1180000/1280000, dictionary size: 616891\n",
      "Parsing tweet 1190000/1280000, dictionary size: 620585\n",
      "Parsing tweet 1200000/1280000, dictionary size: 624302\n",
      "Parsing tweet 1210000/1280000, dictionary size: 627917\n",
      "Parsing tweet 1220000/1280000, dictionary size: 631578\n",
      "Parsing tweet 1230000/1280000, dictionary size: 635273\n",
      "Parsing tweet 1240000/1280000, dictionary size: 638927\n",
      "Parsing tweet 1250000/1280000, dictionary size: 642517\n",
      "Parsing tweet 1260000/1280000, dictionary size: 646079\n",
      "Parsing tweet 1270000/1280000, dictionary size: 649638\n",
      "Parsing tweet 1280000/1280000, dictionary size: 653335\n",
      "Vocabulary size: 2000\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "spacy.require_gpu()\n",
    "from collections import Counter\n",
    "\n",
    "# there are many ways to preprocess the data; this is just one way\n",
    "nlp = spacy.load('en_core_web_sm', disable=['ner', 'tagger', 'parser'])\n",
    "proper_train_tweets = [tweet for tweet, label in proper_train_dataset]\n",
    "histogram = Counter()\n",
    "for idx, doc in enumerate(nlp.pipe(proper_train_tweets)):\n",
    "    if (idx + 1) % 10000 == 0:\n",
    "        print('Parsing tweet %d/%d, dictionary size: %d' % (idx + 1, proper_train_size, len(histogram)))\n",
    "    for token in doc:\n",
    "        lemma = token.lemma_.lower()\n",
    "        if not (len(lemma.strip()) == 0 \\\n",
    "                or token.pos_ == 'SPACE'):\n",
    "            histogram[lemma] += 1\n",
    "\n",
    "vocab_size = 2000\n",
    "vocab = ['<pad>', '<unk>'] \\\n",
    "    + sorted([word for word, count in histogram.most_common()[:(vocab_size-2)]])\n",
    "vocab_to_idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "print('Vocabulary size:', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_tweets(tweets):\n",
    "    n = len(tweets)\n",
    "    all_encoded = []\n",
    "    for idx, doc in enumerate(nlp.pipe(tweets)):\n",
    "        encoded = []\n",
    "        if (idx + 1) % 10000 == 0:\n",
    "            print('Parsing tweet %d/%d' % (idx + 1, n))\n",
    "        for token in doc:\n",
    "            lemma = token.lemma_.lower()\n",
    "            if not (len(lemma.strip()) == 0 \\\n",
    "                    or token.pos_ == 'SPACE'):\n",
    "                if lemma in vocab_to_idx:\n",
    "                    encoded.append(vocab_to_idx[lemma])\n",
    "                else:\n",
    "                    encoded.append(vocab_to_idx['<unk>'])\n",
    "        all_encoded.append(torch.tensor(encoded, dtype=torch.long))\n",
    "    return all_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tweet 10000/1280000\n",
      "Parsing tweet 20000/1280000\n",
      "Parsing tweet 30000/1280000\n",
      "Parsing tweet 40000/1280000\n",
      "Parsing tweet 50000/1280000\n",
      "Parsing tweet 60000/1280000\n",
      "Parsing tweet 70000/1280000\n",
      "Parsing tweet 80000/1280000\n",
      "Parsing tweet 90000/1280000\n",
      "Parsing tweet 100000/1280000\n",
      "Parsing tweet 110000/1280000\n",
      "Parsing tweet 120000/1280000\n",
      "Parsing tweet 130000/1280000\n",
      "Parsing tweet 140000/1280000\n",
      "Parsing tweet 150000/1280000\n",
      "Parsing tweet 160000/1280000\n",
      "Parsing tweet 170000/1280000\n",
      "Parsing tweet 180000/1280000\n",
      "Parsing tweet 190000/1280000\n",
      "Parsing tweet 200000/1280000\n",
      "Parsing tweet 210000/1280000\n",
      "Parsing tweet 220000/1280000\n",
      "Parsing tweet 230000/1280000\n",
      "Parsing tweet 240000/1280000\n",
      "Parsing tweet 250000/1280000\n",
      "Parsing tweet 260000/1280000\n",
      "Parsing tweet 270000/1280000\n",
      "Parsing tweet 280000/1280000\n",
      "Parsing tweet 290000/1280000\n",
      "Parsing tweet 300000/1280000\n",
      "Parsing tweet 310000/1280000\n",
      "Parsing tweet 320000/1280000\n",
      "Parsing tweet 330000/1280000\n",
      "Parsing tweet 340000/1280000\n",
      "Parsing tweet 350000/1280000\n",
      "Parsing tweet 360000/1280000\n",
      "Parsing tweet 370000/1280000\n",
      "Parsing tweet 380000/1280000\n",
      "Parsing tweet 390000/1280000\n",
      "Parsing tweet 400000/1280000\n",
      "Parsing tweet 410000/1280000\n",
      "Parsing tweet 420000/1280000\n",
      "Parsing tweet 430000/1280000\n",
      "Parsing tweet 440000/1280000\n",
      "Parsing tweet 450000/1280000\n",
      "Parsing tweet 460000/1280000\n",
      "Parsing tweet 470000/1280000\n",
      "Parsing tweet 480000/1280000\n",
      "Parsing tweet 490000/1280000\n",
      "Parsing tweet 500000/1280000\n",
      "Parsing tweet 510000/1280000\n",
      "Parsing tweet 520000/1280000\n",
      "Parsing tweet 530000/1280000\n",
      "Parsing tweet 540000/1280000\n",
      "Parsing tweet 550000/1280000\n",
      "Parsing tweet 560000/1280000\n",
      "Parsing tweet 570000/1280000\n",
      "Parsing tweet 580000/1280000\n",
      "Parsing tweet 590000/1280000\n",
      "Parsing tweet 600000/1280000\n",
      "Parsing tweet 610000/1280000\n",
      "Parsing tweet 620000/1280000\n",
      "Parsing tweet 630000/1280000\n",
      "Parsing tweet 640000/1280000\n",
      "Parsing tweet 650000/1280000\n",
      "Parsing tweet 660000/1280000\n",
      "Parsing tweet 670000/1280000\n",
      "Parsing tweet 680000/1280000\n",
      "Parsing tweet 690000/1280000\n",
      "Parsing tweet 700000/1280000\n",
      "Parsing tweet 710000/1280000\n",
      "Parsing tweet 720000/1280000\n",
      "Parsing tweet 730000/1280000\n",
      "Parsing tweet 740000/1280000\n",
      "Parsing tweet 750000/1280000\n",
      "Parsing tweet 760000/1280000\n",
      "Parsing tweet 770000/1280000\n",
      "Parsing tweet 780000/1280000\n",
      "Parsing tweet 790000/1280000\n",
      "Parsing tweet 800000/1280000\n",
      "Parsing tweet 810000/1280000\n",
      "Parsing tweet 820000/1280000\n",
      "Parsing tweet 830000/1280000\n",
      "Parsing tweet 840000/1280000\n",
      "Parsing tweet 850000/1280000\n",
      "Parsing tweet 860000/1280000\n",
      "Parsing tweet 870000/1280000\n",
      "Parsing tweet 880000/1280000\n",
      "Parsing tweet 890000/1280000\n",
      "Parsing tweet 900000/1280000\n",
      "Parsing tweet 910000/1280000\n",
      "Parsing tweet 920000/1280000\n",
      "Parsing tweet 930000/1280000\n",
      "Parsing tweet 940000/1280000\n",
      "Parsing tweet 950000/1280000\n",
      "Parsing tweet 960000/1280000\n",
      "Parsing tweet 970000/1280000\n",
      "Parsing tweet 980000/1280000\n",
      "Parsing tweet 990000/1280000\n",
      "Parsing tweet 1000000/1280000\n",
      "Parsing tweet 1010000/1280000\n",
      "Parsing tweet 1020000/1280000\n",
      "Parsing tweet 1030000/1280000\n",
      "Parsing tweet 1040000/1280000\n",
      "Parsing tweet 1050000/1280000\n",
      "Parsing tweet 1060000/1280000\n",
      "Parsing tweet 1070000/1280000\n",
      "Parsing tweet 1080000/1280000\n",
      "Parsing tweet 1090000/1280000\n",
      "Parsing tweet 1100000/1280000\n",
      "Parsing tweet 1110000/1280000\n",
      "Parsing tweet 1120000/1280000\n",
      "Parsing tweet 1130000/1280000\n",
      "Parsing tweet 1140000/1280000\n",
      "Parsing tweet 1150000/1280000\n",
      "Parsing tweet 1160000/1280000\n",
      "Parsing tweet 1170000/1280000\n",
      "Parsing tweet 1180000/1280000\n",
      "Parsing tweet 1190000/1280000\n",
      "Parsing tweet 1200000/1280000\n",
      "Parsing tweet 1210000/1280000\n",
      "Parsing tweet 1220000/1280000\n",
      "Parsing tweet 1230000/1280000\n",
      "Parsing tweet 1240000/1280000\n",
      "Parsing tweet 1250000/1280000\n",
      "Parsing tweet 1260000/1280000\n",
      "Parsing tweet 1270000/1280000\n",
      "Parsing tweet 1280000/1280000\n"
     ]
    }
   ],
   "source": [
    "proper_train_encoded = encode_tweets(proper_train_tweets)\n",
    "proper_train_labels = torch.tensor([label for tweet, label in proper_train_dataset], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing tweet 10000/320000\n",
      "Parsing tweet 20000/320000\n",
      "Parsing tweet 30000/320000\n",
      "Parsing tweet 40000/320000\n",
      "Parsing tweet 50000/320000\n",
      "Parsing tweet 60000/320000\n",
      "Parsing tweet 70000/320000\n",
      "Parsing tweet 80000/320000\n",
      "Parsing tweet 90000/320000\n",
      "Parsing tweet 100000/320000\n",
      "Parsing tweet 110000/320000\n",
      "Parsing tweet 120000/320000\n",
      "Parsing tweet 130000/320000\n",
      "Parsing tweet 140000/320000\n",
      "Parsing tweet 150000/320000\n",
      "Parsing tweet 160000/320000\n",
      "Parsing tweet 170000/320000\n",
      "Parsing tweet 180000/320000\n",
      "Parsing tweet 190000/320000\n",
      "Parsing tweet 200000/320000\n",
      "Parsing tweet 210000/320000\n",
      "Parsing tweet 220000/320000\n",
      "Parsing tweet 230000/320000\n",
      "Parsing tweet 240000/320000\n",
      "Parsing tweet 250000/320000\n",
      "Parsing tweet 260000/320000\n",
      "Parsing tweet 270000/320000\n",
      "Parsing tweet 280000/320000\n",
      "Parsing tweet 290000/320000\n",
      "Parsing tweet 300000/320000\n",
      "Parsing tweet 310000/320000\n",
      "Parsing tweet 320000/320000\n"
     ]
    }
   ],
   "source": [
    "val_encoded = encode_tweets([tweet for tweet, label in val_dataset])\n",
    "val_labels = torch.tensor([label for tweet, label in val_dataset], dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchnlp.word_to_vector import GloVe\n",
    "pretrained_embedding = GloVe(name='6B', dim=100)\n",
    "\n",
    "embedding_weights = torch.Tensor(vocab_size, pretrained_embedding.dim)\n",
    "for i, word in enumerate(vocab):\n",
    "    embedding_weights[i] = pretrained_embedding[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "proper_train_dataset_encoded = list(zip(proper_train_encoded, proper_train_labels))\n",
    "val_dataset_encoded = list(zip(val_encoded, val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.7790\n",
      "  Validation accuracy: 0.7760\n",
      "Epoch 2 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8053\n",
      "  Validation accuracy: 0.8014\n",
      "Epoch 3 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.7994\n",
      "  Validation accuracy: 0.7945\n",
      "Epoch 4 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8050\n",
      "  Validation accuracy: 0.7995\n",
      "Epoch 5 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8012\n",
      "  Validation accuracy: 0.7949\n",
      "Epoch 6 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8131\n",
      "  Validation accuracy: 0.8065\n",
      "Epoch 7 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8149\n",
      "  Validation accuracy: 0.8085\n",
      "Epoch 8 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8145\n",
      "  Validation accuracy: 0.8075\n",
      "Epoch 9 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8127\n",
      "  Validation accuracy: 0.8051\n",
      "Epoch 10 [==================================================] 1280000/1280000\n",
      "  Train accuracy: 0.8169\n",
      "  Validation accuracy: 0.8094\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEGCAYAAAB/+QKOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3dd3hUZfbA8e9JIQmQAgk9QIAg0ouhNxEVUaTZYLEXVsWytlX357ro6uraG+raG4qKoqgI0qVLRwgloYdQQiCEhCSkvL8/7oSEZAIhU+4kOZ/nmWdm7ty59ySEOXPfcl4xxqCUUkqV5Gd3AEoppXyTJgillFJOaYJQSinllCYIpZRSTmmCUEop5VSA3QG4S1RUlImJibE7DKWUqlRWr1592BhTz9lrVSZBxMTEsGrVKrvDUEqpSkVEdpf1mjYxKaWUckoThFJKKac0QSillHKqyvRBOJObm0tSUhLZ2dl2h1KlBAcHEx0dTWBgoN2hKKU8qEoniKSkJEJDQ4mJiUFE7A6nSjDGkJqaSlJSEi1atLA7HKWUB1XpBJGdna3Jwc1EhMjISFJSUuwORalq74e1+3hx1laS07JoHBHCI0PaMLJrE7cdv0onCECTgwfo71Qp+/2wdh+Pf/8nWbn5AOxLy+Lx7/8EcFuS0E5qpZSqhF6ctfVUciiUlZvPi7O2uu0cmiA8KDU1lS5dutClSxcaNmxIkyZNTj0/efJkuY5xyy23sHXrmf/BJ02axOTJk90RslKqkkhOyzqn7RVR5ZuYzoW72/MiIyNZt24dABMnTqR27do8/PDDp+1jjMEYg5+f81z98ccfn/U8EyZMqHCMSqnKJScvn/cW7qCspd4aR4S47Vx6BeFQ2J63Ly0LQ1F73g9r97n9XImJiXTo0IE777yTbt26sX//fsaPH09cXBzt27fn6aefPrVvv379WLduHXl5eURERPDYY4/RuXNnevfuzaFDhwB44okneO21107t/9hjj9GjRw/atGnD0qVLAcjMzOSqq66ic+fOjB07lri4uFPJSylVOSxOOMzQ1xbx8uxtdIkOJzjg9I/wkEB/HhnSxm3nqzZXEE/9tIn45PQyX1+7J42T+QWnbcvKzefvUzfw1R97nL6nXeMw/nVl+wrFEx8fz8cff8y7774LwPPPP0/dunXJy8tj0KBBXH311bRr1+609xw7doyBAwfy/PPP8+CDD/LRRx/x2GOPlTq2MYY//viD6dOn8/TTTzNz5kzefPNNGjZsyHfffcf69evp1q1bheJWSnnfofRs/v3LZn5an0zzyJp8ckt3LmxTX0cxeUvJ5HC27a5q1aoV3bt3P/X8q6++4sMPPyQvL4/k5GTi4+NLJYiQkBCGDh0KwAUXXMCiRYucHnv06NGn9tm1axcAixcv5tFHHwWgc+fOtG9fscSmlPKevPwCPl++m5d/28bJ/AL+dnFr7hzYiuBAf8AareTOhFBStUkQZ/um3/f5eexz0rnTJCKEr//a2+3x1KpV69TjhIQEXn/9df744w8iIiK4/vrrnc7+rlGjxqnH/v7+5OXlOT12UFBQqX2MKavFUinli9bsOcoT0zYSvz+dAefV46nh7WkRVevsb3Qj7YNweGRIG0IcWbmQu9vzypKenk5oaChhYWHs37+fWbNmuf0c/fr145tvvgHgzz//JD4+3u3nUEq5Lu3ESR7//k+uemcpRzJP8va4bnx6S3evJweoRlcQZ1N4mebJ9ryydOvWjXbt2tGhQwdatmxJ37593X6Oe++9lxtvvJFOnTrRrVs3OnToQHh4uNvPo6ouT7d3V3cFBYapa5J4/tctHMvK5ba+LfjbJedRO8i+j2mpKk0PcXFxpuSCQZs3b6Zt27Y2ReRb8vLyyMvLIzg4mISEBC699FISEhIICKjYH5/+bquXkrN2wbrCfm50R00SbrDlQDpPTNvIqt1HuaB5HZ4Z2YG2jcK8cm4RWW2MiXP2ml5BVBMZGRkMHjyYvLw8jDH873//q3ByUNWLMYb/zNjsdNbukz9u5FhWLrWDAqgVFOC496d2UAC1g61ttWoE4O/nnvIsVe0qJiMnj9fnbOOjJbsICw7ghas7cXW3aPzc9PtylX5CVBMRERGsXr3a7jBUJbH/WBZLElNZmniYpdtTOXQ8x+l+6dl5/Gv6prMeLyTQn1pBAYQGWwmkVo2AoqQS7Hhcoyi5lNxeOyiA3xMO8cwvm8nOtUYWeqL2kLcYY5i58QBP/RTPgfRsxvZoyt+HnE+dWjXO/mYv0gShlCLtxEmW70hlSWIqS7YfZkdKJgB1agbSJzaKJQmHScvKLfW+xuHBTL+3H5k5eWTk5JGZk09mTh7Hc/LIdNwyit1nOF7PyMnjQHp2se15pz74z0VWbj7P/bq5UiWIXYcz+df0TSzclkLbRmFMGteNC5rXsTsspzRBKFUNZZ3MZ+WuIyzZfpilialsTD6GMVCzhj89WtRlbPdm9ImNpG3DMPz8pMw+iL9fdj5RtYOIqh3kckx5+QVknsw/lVhOTzL5PPzteqfvO5iew4UvzqdvbBT9W0fRu2UU4TV9bzGr7Nx83l24nbcXbKeGvx9PDmvHjb2bE+Dvu4NJNUEoVQ3k5hewISnNukJIPHyqckCgv9C1aR3uH9yavrFRdI6OoEZA6Q8sb4zyC/D3IzzEj/AQ5x/ur87e5nSuUnhIAK3q1eaHtfuYvGIPfgIdoyPoHxtF39goujWPICjA38kRvef3bSk8+eNGdqWeYFinRvxzWDsahAXbGlN5aIJQqgoqKDBsPXicJY4+hBU7Usk8mY8ItGsUxs19Y+jTKpIeLepSs0b5PgY8PWv3bB4Z0sbpVcxTwzswsmsTcvMLWLc3jUUJh1mSeJh3Fm7nrfmJhAT607NlXfrFRtGvdRRtGoR6bU2TA8ey+fcv8fyyYT8tomrx+W096N+6nlfO7Q6aIDzswgsv5PHHH2fIkCGntr322mts27aNt99+2+l7ateuTUZGBsnJydx3331MnTrV6XFfeukl4uKcjk47dZ7x48dTs2ZNAC6//HK+/PJLIiIiXPyplC/ak3qCJdutD8dl21NJzbRKyreIqsXIrk3oGxtF75aRPtcRWl5nu4oJ9Peje0xdusfU5cFLziM9O5fl260rpkWJh3nml80ARNUOol9sJP1a16NfbBQNw93/TT4vv4BPlu7i1dnbyC0wPHjJeYwf0PJUiYzKQhOEM/Ofg0GPu+VQY8eOZcqUKacliClTpvDiiy+e9b2NGzd2mhzK67XXXuP6668/lSBmzJhR4WMp+5Q1tDPleA5LHX0IS7YfJumo1fxSPzSIAefVo0+rSPrERtHEjeWf7XYuVzFhwYFc2r4hl7ZvCFjrJCxOPMzihMMsSjjMD+uSAYitX5t+jv6Lni0jXZ6Ytnr3Ef5v2ka2HDjOhW2sEhnNI70/C9odNEE4s/B5tyWIq6++mieeeIKcnByCgoLYtWsXycnJdOnShcGDB3P06FFyc3N55plnGDFixGnv3bVrF8OGDWPjxo1kZWVxyy23EB8fT9u2bcnKKmqLveuuu1i5ciVZWVlcffXVPPXUU7zxxhskJyczaNAgoqKimD9/PjExMaxatYqoqCheeeUVPvroIwBuv/12/va3v7Fr1y6GDh1Kv379WLp0KU2aNOHHH38kJKTqfMBUNs6WlXzo2/U8/+tmDqRbQ09DgwPo1TKSO/q3pG9sJK3q1dZlYZ1oHBHCtXFNuTauKQUFhi0HjrM4MYXFialMWbmHT5buIsBP6Nos4lSHd+foiHJ3Ih/JPMl/f93C16v20ig8mHev78aQ9g0r9b9F9UkQvz4GB/4s//4fX3H2fRp2hKHPn3GXyMhIevTowcyZMxkxYgRTpkzhuuuuIyQkhGnTphEWFsbhw4fp1asXw4cPL/OP6Z133qFmzZps2LCBDRs2nFau+9lnn6Vu3brk5+czePBgNmzYwH333ccrr7zC/PnziYqKOu1Yq1ev5uOPP2bFihUYY+jZsycDBw6kTp06JCQk8NVXX/H+++9z7bXX8t1333H99def/XehPMLZspL5BYajJ3J5ZEgb+sZG0aFxmE+PhPFFfn5Cu8ZhtGscxvgBrcjOzWfN7qPWFUbiYV6fm8BrcxIIDQqgZ8tI+re2+i9aRtVCRE67qmsUEUz/2ChmxR8kIzuPvw5oyX2DW1PLxhIZ7lL5fwJ3SdsNx/YWPd+92LoPbwoRzV06dGEzU2GC+OijjzDG8I9//IPff/8dPz8/9u3bx8GDB2nYsKHTY/z+++/cd999AHTq1IlOnTqdeu2bb77hvffeIy8vj/379xMfH3/a6yUtXryYUaNGnaooO3r0aBYtWsTw4cNp0aIFXbp0AU4vF67sUdbykSfzCpgwKNbL0VRdwYH+9ImNok9sFH8HjmaeZNmOVBYlHGZxYgpzNh8ErHkf0XVCWLs3jdx8q0xRclo2X69KokVUTb4e35s2DUNt/Encq/okiLN80z/NxHCYeMxtpx45ciQPPvgga9asISsri27duvHJJ5+QkpLC6tWrCQwMJCYmxmmJ7+KcXV3s3LmTl156iZUrV1KnTh1uvvnmsx7nTPW3CkuFg1UuvHhTlvK+BmHBHEgv/e/pzmUlVWl1atXg8o6NuLxjI8AaALAoMYUliYf5deMBnP0XOplXUKWSA2i5b6+oXbs2F154Ibfeeitjx44FrNXh6tevT2BgIPPnz2f37t1nPMaAAQOYPHkyABs3bmTDhg2AVSq8Vq1ahIeHc/DgQX799ddT7wkNDeX48eNOj/XDDz9w4sQJMjMzmTZtGv3793fXj6vcKLpO6RE23ipDr4o0i6zJuJ7NeXvcBZS1GHRy2pm/mFVGmiCcGVh6GU9XjR07lvXr1zNmzBgAxo0bx6pVq4iLi2Py5Mmcf/75Z3z/XXfdRUZGBp06deKFF16gR48egLU6XNeuXWnfvj233nrraaXCx48fz9ChQxk0aNBpx+rWrRs333wzPXr0oGfPntx+++107drVzT+xctXKXUdYtTuNS9rWp0lECIK1gJVWULVXWVdvVfGqTst9qwrR361n5RcYhr25mGMnTjLnoYHlnsymPK+qlT7Xct9KVTJfrtjN5v3pTPpLN00OPsbOxcW8Tf/ylPIxRzJP8tJv2+jTKpLLOzof1absZXfZEW+p8n0QVaUJzZfo79SzXpy1lcycPJ4a3r5ST7JSlV+VThDBwcGkpqbqB5obGWNITU0lONj3K1FWRhuS0piycg839YmhdYOqNWRSVT5VuokpOjqapKQkUlJS7A6lSgkODiY6OtruMKqcggLDkz9uIrJWEPdf3NrucJSq2gkiMDCQFi1a2B2GUuUydU0S6/am8fI1nQkL9r0Fb1T1U6WbmJSqLI5l5fLCzC1c0LwOo6pB56eqHDyaIETkMhHZKiKJIlJq9pmINBOR+SKyVkQ2iMjlju2Rju0ZIvKWJ2NUyhe8NmcbqZkneWp4e/z8tGNa+QaPJQgR8QcmAUOBdsBYEWlXYrcngG+MMV2BMUDhCjrZwD+Bhz0Vn1K+YsuBdD5btpu/9GhGhybhdoej1CmevILoASQaY3YYY04CU4ARJfYxQJjjcTiQDGCMyTTGLMZKFEpVWcYY/vXjJkKDA3j4Uq2vpHyLJxNEE6BY/WySHNuKmwhcLyJJwAzg3nM5gYiMF5FVIrJKRyqpyujnDftZsfMIjwxpU2mXAlU+YP5zHjmsJxOEs4bUkhMSxgKfGGOigcuBz0Wk3DEZY94zxsQZY+Lq1as8C4ErBZCZk8ezv2ymQ5MwxnRvZnc4qrLKSLFWwfQATyaIJKBpsefROJqQirkN+AbAGLMMCAaiUKoaeGt+IgfSs3lqeHv8tWNanauCAlj1EbwVV/TczTyZIFYCrUWkhYjUwOqEnl5inz3AYAARaYuVILStSFV5O1Iy+GDRDq7qFs0FzevaHY6qbPavh1fbwc8PQHaate3pOtZiZ25sbvLYRDljTJ6I3APMAvyBj4wxm0TkaWCVMWY68BDwvog8gNX8dLNx1MUQkV1YHdg1RGQkcKkxJt5T8SrlLcYYnvopnuAAfx4dWgk7puc/B4MetzuK6ik7HeY/C3+8BzUjYfT70PEaeCrCratgFvLoTGpjzAyszufi254s9jge6FvyfY7XYjwZm1J2mbP5EAu3pfDEFW2pH1oJa1otfF4ThLcZA5u+h5n/gIyD0P02uOifEBLh0dNW6VIbSvma7Nx8nv55E63r1+amPjF2h1N+2emwcyHsWW493zQN6rSAui0hOOzM71WuSd0OvzwEO+ZDoy4w9ktocsHp+3hgFUzQBKGUV733+w72Hsniy9t7Eujvw5VujIFD8ZAwGxLnwK7FnDYI8dubix7XjLQSRd2WRUmjruO+ZiRoyfKKyc2Cxa9at4BguPwliLsV/PxL7+uhKzpNEEp5yd4jJ5g0P5ErOjWiT6wPDtbLPgY7FjiSwlw47hh02KAD9L0fWl8CTXvCv6PgzsVwZAcc2WndH90Ju5fChm84LZHUCHUkixalE0hoY/Dz4SRpp4Q5MONh6/fa8Rq49FkIbeD1MDRBKOUlz/6yGT8R/u9yH1nL2xg4uAkSZ1sfSHuXQ0EeBIVBywuthBB7MYQ1Lv3ehh2tW0l5OXB0t/XBVjyBHNwEW2ZAQW7Rvv5BUCfm9CuOOo5kEtEM/M9Q0baqdpSnJ8PMxyD+R4hsDTdOh5YDbQtHE4RSXrAoIYWZmw7wyJA2NI4IsS+QMq8SOkKfeyH2Emja48wfzmdq7w4IgnrnWbeSCvLhWFLRFcepBLLT6t/IPVG0r/hDRNPSTVZ1WlhJpap1lOfnwYp3YcFzVpK+6Anoc5/1+7SRVJXV1uLi4syqVavsDkOpUk7mFTD09d/JLzDMemAAQQFO2pA9xRg4uLGoL2HvCsdVQji0utBKCLEXQ1gj78VUVpwZB0s3Wx3ZYd2ynQzh7HkntB8F0T0qd1PVnhXwy4PWv1PrIXD5C1YS9BIRWW2MiXP2ml5BKOVhnyzdyfaUTD6+ubt3kkNWmnWVkFh4lbDf2t6wo/WttPUlEN39zFcJ3iYCoQ2tW/M+pV8/cQTm/AvWfFa0bcW71q1GKHS70ZEs4ipPp3jxnymsCVz3BZw/zKfi1ysIpTzoYHo2F720gN6tIvngpu6eOYkxcODPYn0JK8DkO64SBhX1JYQ29Mz57TAxHB5Pgq0zrSG3ibMh/ySEN4X2I61k0bibT33YnlJQAOsmw+wnIScdet0NAx+FoNq2hKNXEErZ5LkZm8ktMPxzWMmlUCqgeMdsVpo1Lj5hjtV0lHHA2t6wE/T7m9V0FN0d/Kvwf/GgUOh0jXXLPgZbf7WSxfJ3YembVkd3+1HQfjQ06uwbyeLgJvj5QWtAQLPecMUr0MANfxseUoX/epSy1x87j/DDumTuvSiW5pG1XDuYMVbHrH+goy/hD+sqITgcWl3k6EsYXLWuEs6kZEd5cDh0HmPdstJgyy9Wslg2CZa8bnVutx9l3Rp29H6yyMmw/v2WvW3FOmISdP6Lz/edaBOTUh6Ql1/AsDcXczw7jzkPDiSkhgt9DznH4cMhcGiT9bxRZyshtL4EmsRV7asEV5044kgW38OOhVZSjYwtShb123k2WRgDm3+yhq6m74NuN8HFE6Gm7xRo1CYmpbzsyz/2sOXAcd4Z18215DD/udK1/vevh/OGQrNergVZHdSsC91usG6ZqbDlJ9j4PSx6GX5/EaLOK2qGqn++e899ZCf8+ndI+M0aRnzNJ9YQ4kpEryCUcrPUjBwGvbSAjtHhfHFbT8SVb6gH4+HdftB1nDXaxQMVO6uljBTYPN1qhiosI1KvrZUsOoyGqNYVP3ZeDix5Axa9BH4BMOgf0OOvPnulp1cQSnnRS79t5cTJfCZe2d615GCMVW4hOAwGTzx9iKdyTe16VkXU7rfB8YNFyWLBc7DgP1Z5kfYjrSuLyFblP+6OBfDLw5CaAO1GwmXPOZ+JXklogvABP6zdx4uztpKclkXjiBAeGdKGkV1LLt+tKoP1e9OYsnIvt/drQesGoa4dbMPXsHsJXPk61Ir0WMXOai+0AfS4w7ql77eSxcbvYd4z1q1hJ0cz1EhrNndxhSPLjh+EWf+AjVOtDvFx30Hri+35edxIm5hs9sPafTz+/Z9k5eaf2hYS6M9zoztW2yRRWRNmQYFh1DtLSU7LYt5DAwkNdmEiWlaatZRkRHO4bbbPj3apko7ts2oibZoGSX9Y2xp1sZqg2o2EOs2t+RhDX4R5/4a8bOj3IPR7AAIrzzof2sTkw16ctfW05ACQlZvPi7O2VooPRXcrmTD3pWXx+Pd/Avj872Pq6iTW703jlWs7u5YcwFo17EQqjJuqycEu4U2g993WLW1PUbKY/aR1K1yT4ddHoOUguOLlc2uOqgT0L89myWlZ57S9qjtTwvRlx7Jy+e/MLcQ1r8MoVxNZ8jpY+QF0vx0ad3FPgMo1Ec2sYoZ3zLNqQAHsW130+o75jlLnVYteQdiscUQI+5wkA38/YXdqpusTrCqZshLjvrQsjDGudfp60Kuzt3H0xEk+G9HDtRgLCqzVw2pGwaD/c1+Ayn2G/te6gdXEVIVHlukVhM0eGdKG4MDT/xlqBPgR6C+MnLSEFTtSbYrM+2ZtOnDG1//y/grW703zUjTlt3l/Op8t28W4ns1p3zjctYOt/Qz2rYJL/+3x9YaVOhtNEDYb2bUJ18Y1BUCAJhEhvHBVJ2bcP4A6tWpw/Ycr+GblXnuD9LCCAsMrs7fx189XE10nhOCA0/8sgwP9GN21MVsPHmfEpCVM+HINOw9n2hTt6Ywx/Gv6JsJDAnnoUidrIJyLzFSYMxGa94VO17klPuVhVXxkmTYx+YCc3AJCgwNY9+Sl+PsVNU9Mu7sv93y5hr9/t4HElAwevez8016vCtKzc3nw63XM2XyIqy+I5pmRHZi58YDTUUzHs3N5f9FOPli0g1kbDzC2RzPuHRxL/VD7Roz8tGE/f+w8wn9GdSSiZg3XDjZ3ImSnW2sP+2hTmiqhKi1a5IQOc7WZMYZ+/51PhyZh/O+G0iPN8vILePrneD5btpvB59fn9bFdqR1UNfJ64qEMxn++ij2pJ3jyynbc0Kt5udrvDx3P5s25iXz1xx5qBPhxe/+W3NG/hesjh85RZk4eF728gPqhwfwwoa9ryXvvSvjwYqsj9NJn3BekUmdxpmGu2sRks92pJ9iXlkW/1vWcvh7g78fTIzrw9Ij2LNiWwlVvL2XvkRNO961MZscfZOSkJRw7kcvk23tyY++Ycnfu1g8N5t8jOzD7wYEMOr8+b8xN4MIXF/DJkp2czCvwcORF3pyXyMH0HJ4a0d615JCfB788AKGNq3yThapcNEHYbHHiYQD6xUadcb8be8fwyS3dST6WxchJS1i164g3wnO7ggLDq7O3ccdnq2hZrxY/3duPni0jK3SsFlG1mPSXbvw4oS/nNQhl4k/xDH5lAT+u20dBgWevjLenZPDh4h1cfUE03ZrVce1gqz60Fvy57D+2LRqjlDOaIGy2OOEwTSJCiImsedZ9+7eux7S7+xIaHMBf3l/B92uSvBCh+xzPzmX856t5fW4CV3WL5pu/9qZxRIjLx+3cNIIv7+jJp7f2oHZQIPdPWceVby1mUUKKG6IuzRjDUz/FExzgz6OXuVgB9PhBq5xDy0HW7FylfIgmCBvlFxiWbj9Mv9iocjevxNavzQ8T+nJB8zo8+M16Xpi5xePflt1he0oGIyctYf7WQ0y8sh0vXdOJ4ED3rc8sIgw8rx6/3NuP167rwrGsXG748A+u/2AFfya5d5z67PiD/L4thQcuOY96oUEuHuyfVokG7ZhWPkgThI027jtGenYefVufuXmppIiaNfjsth6M7dGUtxds584vVpOZk+ehKF03J/4gI99aQpqjv+Hmvi08NuHNz08Y2bUJcx8ayJPD2rEp+RhXvrWYe79ay+5U14fGZufm8/TP8bRpEMqNvZu7drBdi62CfH3ug6hYl2NTyt00QdiosP+hT6tzb4MP9PfjP6M68uSwdszZfJBr3l3mc+U5CgoMr89J4PbPVhETVYvp9/ajVwX7G85VUIA/t/ZrwcK/D+KeQbHMjj/A4JcX8q8fN3I4I6fCx3134XaSjmYxcXh7Avxd+O+Tn2vNmI5oBv0fqvhxlPIgTRA2WpxwmLaNwoiqXbFmChHh1n4t+PDm7uw5coLhby1h7Z6jbo6yYo5n53LnF6t5dc42Rndtwrd39qaJG/obzlVYcCAPD2nD748M4rruTflixR4GvjCf1+ZsI+Mcr7r2HjnBOwu2M6xTI3pXIKmfZvnbkLIFhr4ANc7e/6SUHTRB2CTrZD6rdx+l/zk2LzkzqE19vr+7DyE1/LjuveX8uG6fGyKsuB0pGYx6eylztxziyWHtePnazm7tb6iI+mHBPDuqI789MICBberx2pwELnxxPp8t21XuobHP/BKPnwj/d0Vb14I5tg8W/NdaNrTNUNeOpZQHaYKwycpdRziZX0DfswxvLa/zGoTy44R+dImO4P4p63jlt622dF7P23KQEW8t4UjmST6/rQe39vNcf0NFtKpXm7fHXcC0u/vQql5tnvxxE5e8upCf1ief8fe1cFsKszYd5N7BsTQKd/FKaNbjYPJh6PNn31cpG2mCsMmSxMPU8Peje0yxMfTzn3PpmHVr1eDz23twzQXRvDEvkXu+WkPWyfyzv9ENCgoMb85N4LZPV9EssibT7+lLn1buSX6e0LVZHaaM78XHt3QnJNCfe79ay4hJS1ji6Bcq7mReAU9N30SLqFrc1q+FaydOnGutK9D/YagT49qxlPKwqlGzoRJalHCYbs0jqFmj2D/Bwuddru0SFODPC1d3onWD2jz36xb2HlnG+zfG0TDcc/WKMnLyeOibdczadJBRXZvw3OiOtjcplYeIMKhNfQa0rseP6/bx8m/bGPfBCvq3juLRy84n8VAGL87aeqoc+/gBLQgKcOHnysuBGY9A3VbQ9z43/RRKeY4mCBukZuQQvz+dR4a0KdpYWBOrIB/8XPtwFRHGD2hFy6ja3D9lLcPfWswHN5s/iKEAAB+vSURBVMXRKdr95aN3Hs5k/Ger2HE4kyeuaMttPtakVB7+fsLobtFc3rERXyzfzVvzExn25mL8RcgvVqvs82V7aNcovOIr2y15A45shxumQYCL8yeU8gJtYrLB0u3WGg99Y6OsZqWJ4fCU48P76brWcxebmwAubteAqXf1IdDfj2v/t4xfNux3+ZjFzd9yiOFvLeZwRg6f39qD2/u3rHTJobjgQH9u79+S3/8+iNpBAaclB3BxZbuju2DRS9Zs6VYXuR6sUl6gCcIGixMOExocQMcm4VaT0sRjcMHNRTt0uBoGPOKWc7VtFMYPE/rSrlEYE75cwxtzE3C1gq8xhknzE7n105U0rVOT6ff0o4+bOtt9QVhwYJkTDys81+TXR0H8Ych/XIhMKe/SBOFlxhgWJx6mT6vIogqgxkDiPOvx4H/Bxqnw3W3WZCo3qBcaxJd39GJU1ya8Mnsb901ZR3ZuxTqvM3PyuHvyGl6ctZXhnRvz3V19aFq36o3jL6tGVIVqR22ZAdtmwoWPQbiL61Ur5UWaILzMaXnv1O1wbA/EXgL9H7TWA4j/AabeAnkn3XLe4EB/Xrm2M48MacNP65O57r3lHErPPqdj7Dqcyai3lzBr0wGeuKItr13XhZAavt8ZXRGPDGlDSImO9pBA/9P7jcrj5Anr6qFeW+h1lxsjVMrzzpogROQeEalQPWMRuUxEtopIooiUKnQvIs1EZL6IrBWRDSJyebHXHne8b6uIDKnI+X2R0/Le2+da95e/YN33uRcuex42/wTf3myNfnEDEWHCoFjevf4Cth2wlu/cuK98hewWbLX6Gw4dz+GzW3tW+v6GsxnpGI3VJCLk1FKwz43ueO4d1ItetpL/FS+Dv3cXNFLKVeUZxdQQWCkia4CPgFmmHI3YIuIPTAIuAZIcx5hujIkvttsTwDfGmHdEpB0wA4hxPB4DtAcaA3NE5DxjjHcG9XuQ0/Le2+dBnRZQt2XRtl53gV8AzHgYvr4Brv0MAt0zVPWyDg2JrtObOz5bxTXvLuO1MV0Y0r6h032NMbyzcDsvztrK+Q3DeO+GC6pkk5IzI7s2qfiIJYDDCbD0Deg0BmL6ui8wpbzkrFcQxpgngNbAh8DNQIKI/EdEWp3lrT2ARGPMDmPMSWAKMKLk4YEwx+NwINnxeAQwxRiTY4zZCSQ6jlepOS3vnXcSdi5yPrKlxx0w7FVImAVfj4Pcc2sSOpMOTcKthXYahvLXz1fz9oLEUp3XmTl53PPlWl6YuZVhnRrzvbf7G9wwkss2xljJPSAELv233dEoVSHlmgdhjDEicgA4AOQBdYCpIjLbGPP3Mt7WBNhb7HkS0LPEPhOB30TkXqAWcHGx9y4v8d5SX+VEZDwwHqBZs2bl+VFs5bS8997lkJsJsYOdvynuVutKYvp98NUYGPOl24q71Q8L5uvxvXhk6gZemLmVeZsPkpyWzf5j2dQPC0KAQ8dz+Mfl53OHHU1Kbpg4aJtN02DHAhj6ItSub3c0SlVIefog7hOR1cALwBKgozHmLuAC4KozvdXJtpJNU2OBT4wx0cDlwOci4lfO92KMec8YE2eMiatXz/mazr7EaXnv7fOsBBDTv+w3drsRRkyyPnC+ug5Our6uQaHgQH/eGNOFoR0asmp3GsnHsjHAwfQcDqTnMH5AS8YPaOX95JCV5t3zuVPOcZj1D2jYCbrfZnc0SlVYeUYxRQGjjTFDjDHfGmNyAYwxBcCwM7wvCWha7Hk0RU1IhW4DvnEcbxkQ7Dhfed5b6Tgt7504F6J7QHBY2W8E6DoORv3PWmRm8rWQk+G2uESEDWWsuvbTevdOrjurwomD/3UsxjMx3G0TB71mwfNw/IDVPOjirHil7FSeBDEDOFL4RERCRaQngDFm8xnetxJoLSItRKQGVqfz9BL77AEGO47bFitBpDj2GyMiQSLSAqsP5I/y/Ui+yWl574xDcGADxJZzZm3n62D0+7BnGUy+2vqm6iZlTQDz6iJEeTlw0pH46hVb67l+ezjvUu/F4YqD8bD8HeuqLzrO7miUckl5EsQ7QPGvq5mObWdkjMkD7gFmAZuxRittEpGnRWS4Y7eHgDtEZD3wFXCzsWzCurKIB2YCEyr7CCan5b13LLDuW5XR/+BMx6vh6g9h7x/w+WjIds96y26dGFYRKVvhg8Gw7C3ofgeMX2BtH/s1ZB2FDy6G3/4Jub61at5pjLFWiQsOh4sn2h2NUi4rTye1FB/WaowpEJHydm7PwLoCKb7tyWKP4wGn4/+MMc8Cz5bnPJWB0/LeiXMhpC406nxuB2s/yirbMPUW+HwUXP89hLhWiO+RIW14/Ps/ySo2w7pCE8POlTGw6iOY9X9W5/vYr6HNZdZrAx+zHjfvbSWHpW/All9gxFvQvI9n46qI9VNgz1K48g2oWdfuaJRyWXmuIHY4OqoDHbf7gR2eDqyqKVXeu6DA6qBuNahi7dTthltzI/ZvgM9GwIkjZ3/PGbhtYti5yEyFKePglwetD/y7lhUlBygawRQcDsPfgBunQ0EefDwUfnnYrU1sLstKg9n/hOju0PUGu6NRyi3KcyVwJ/AG1qQ2A8zFMbRUlY/T8t6HNkHmoXNrXirp/CtgzGT4+nr4bLj1AerCN1eXJ4adi+3zYNpdkHUEhjwHPe8Ev7N8X2k5EO5eBnP/DSveteobXfl62UOEvWneM3AiFa7/7uw/h1KVRHkmyh0yxowxxtQ3xjQwxvzFGHPIG8FVFaeV9y6U6Civ0WqQawc/bwiM+QpStsGnV0Jm6RXRfEpejtWc9Pkoq1nsjnnQ++7yf6jWqGUt1XnrLAgMgS9Gww8TrH4KuySvg1UfWn0n59pcqJQPK888iGARmSAib4vIR4U3bwRXVZxW3rvQ9rlQvx2ENXb9BK0vhr9MgdRE+GSYNTrKF6VsK9YRfTvcMR8adqzYsZr1hL8ugv4PwfqvYFJP2Pyze+Mtj4ICq4msZhRc9H/eP79SHlSer22fY9VjGgIsxJqT4EONv77NaXnvk5mwZ7l7F45pdRH85RtI2w2fXGGNw/cVhR3R/xsA6cnWFc8VL7s+IzwwGAY/aV2F1KpvlSP59mbISHFL2OWy5lPYt9qqwBscfvb9lapEypMgYo0x/wQyjTGfAlcAFfzaV/04Le+9awnkn3T/ymItB8K4b+HYPitJpPvA3MLCjuifH7BGI921FM6//OzvOxeNu8D4+XDRE9Yop0k9YMO3Rcu4ekpmKsx9Cpr3g07XevZcStmgPAmicNWaNBHpgFVUL8ZjEVUxZZb3Dgj2zFDNmH5WR+nxA1aSOJbk/nOU1/b58E4fSJxtraQ27jsIdV411mX+gdYqfH9dZFXF/f52q3aVJ5PknH9ZI6mueAmqcOlzVX2VJ0G851gP4gmsGc7xwH89GlUVUmZ57+Z9rU5WT2jeG26YZnVYf3w5pO3xzHnKkpcDvz0Bn4+0ml1unwu9J3hndE/98+G236yEtGOh1Tex+hP3X03s/QPWfg697ob6bd17bKV8xBn/xzoK56UbY44aY343xrR0jGb6n5fiq9QKy3v3jY0sKnaXthcOb/P8wvVNe8ANP1jj8z++Ao7u8uz5CqVss2Y9L30T4m6zZkQ36uSdcxfy87cS0t1LrVFFP91vDQM+stM9x8/Pszqmw5rAwEfdc0ylfNAZE4SjIN89Xoqlyiks731a/0Ph6nHeGLsffQHc9CPkpFtJ4ogH5zcW74g+lmR1RA97xW2lySukbktrbsiw12DfWqu5a/k7UOBi1ZZVH8KBP62rlKDa7olVKR9Unmv+2SLysIg0FZG6hTePR1YFlFneO7Tx6cXoPKlxV7hpOuSesJJE6nb3n+PEEWuy3s8PQLNe1mQ2d3dEV5SfH8TdAhOWW/0zMx+Djy6zaj9VxPED1qS4VhdBu5LrXylVtZQnQdwKTAB+B1Y7bqs8GVRVUaq8d36eVaCv1UXe7dRs1Blu+gnyc6w+icMJ7jv2jgXWN/Nts+DSZ626UJ7qiHZFeLQ1DHjUe5CaAO/2g99fgvzcs7+3uN/+CXnZcLl2TKuqrzwzqVs4ubU82/uqO6flvZPXWNVXy1ve250adoCbfgaTbyWJQ1tcO17eSevD8rOREBQKd8yFPvf4dpkJEatk+oQ/oM1QmPdveP8iq55VeexcBH9+A33vh8izrbirVOVXnpnUNzq7eSO4ysxpee/t8wCBli6W16ioBu3g5l+sD8pPrrDWLqiIlG3w4cVWddULbobxCytXiYna9a1Ch9d+bjUZvT/Iqu+Ul1P2e/JzrTWmI5pbs7eVqgbK83Wve7Fbf6x1pIef6Q3qDOW9G3e1txR0vTZWkvAPhE+HWZ2t5WUMrPrY6ohO2wvXTYYrX7O3I9oV7YbDhBXQ8RpY9JL1c+1d6Xzf5W9DyhYY+oLnhicr5WPK08R0b7HbHUBXoIbnQ6vcSpX3zjoK+1b5RuXRqNZWkggItgr8Ja87+3tOdUT/zRpCe9dSaHumFWcriZp1YdS7MG6qtYzrh5fAzH/AyRNF+xxLspYRbXP56eXIlariKtJgfAJrCVBVhsLy3v2LD2/d+TuYAtfKe7tTZCsrSdSobc0R2Lem7H2Ld0Rf8m9rfkVYI6+F6hWtL7FGX8XdCssnwTu9rX8zsCrPGgOXPW9vjEp5WXn6IH4SkemO28/AVuBHz4dWeZVZ3rtGqG+tU1y3hZUkgsOtzuakEoPTindE16gNt8+Bvvf5dke0K4LDrLkbN/8C4mddXX15nTWxccBDUKe53REq5VXlWTDopWKP84DdxhgbC/z4vlLlvY2xOqhbDrTa/n1JneZw8wyrP+KzkVYdp+3zrLWvv7sd9q+zOqKH/Mdai6E6iOkHdy6B+c9afQ8Afe6zNyalbFCeBLEH2G+MyQYQkRARiTHG7PJoZJWU0/LeqYlwbC/0e8De4MoS0bQoSXwxGk5mWCOUAoLgui+g7ZV2R+h9S1631q0o9Ex9637gY0VLoSpVxZWnreBboKDY83zHNuWE0/Lep1aPs2H+Q3mFN7GSRKijbyE6ztERXQ2TA1hJYOIx6wZFjzU5qGqkPAkiwBhzsvCJ47GOYipDmeW967a02vx91fzn4JXzrVnGYHXQvtLW2q6UqpbK08SUIiLDjTHTAURkBODjCx/bp1R577wc2LUYuoyzN7CzGfR40bfjieFF35yV1aykVDVUngRxJzBZRAobZJMAnUntRGF578s6NCwq771nuVUoz5ebl9SZabOSqqbOmiCMMduBXiJSGxBjjK5HXYYyy3v7BUCL/vYFdq70G7NSivLNg/iPiEQYYzKMMcdFpI6IPOON4CqbMst7N+1lFbSrLPQbs1KK8nVSDzXGpBU+McYcBXyk2L9vKVXeO+OQVeuolU3F+ZRSygXlSRD+IhJU+EREQoCgM+xfLRWW9+4XW+LqAXyj/pJSSp2j8nRSfwHMFZGPHc9vAT71XEiVU2F579P7H+ZBzShoWIlKYSullEN5OqlfEJENwMWAADMBLUpTQqny3gUFVoJoNajq1i5SSlVp5f3kOoA1m/oqYDCw2WMRVVKlynsf/BMyU3R4q1Kq0irzCkJEzgPGAGOBVOBrrGGu2uNaQmF570eGtCnaWNj/oAlCKVVJnamJaQuwCLjSGJMIICI+Wm3OXmWW927QAUIb2hSVUkq55kxNTFdhNS3NF5H3RWQwVh+EKqFUee+cDGsGtQ5vVUpVYmUmCGPMNGPMdcD5wALgAaCBiLwjIpd6KT6f57S89+4lUJDrO6vHKaVUBZRnTepMY8xkY8wwIBpYB2gtBocyy3sHhECz3vYFppRSLjqn8ZfGmCPGmP8ZY7Tn1aHM8t4xfSEw2KaolFLKdTpA30Wlynsf3W2tIKfNS0qpSk4ThAsKy3v3jY0sKu+t5TWUUlWERxOEiFwmIltFJFFESvVbiMirIrLOcdsmImnFXvuviGx03K7zZJwVVWZ577AmEHWefYEppZQblKcWU4WIiD8wCbgEa5GhlSIy3RgTX7iPMeaBYvvfC3R1PL4C6AZ0wSoMuFBEfjXGpHsq3oooVd47Pw92/A7thoPoiGClVOXmySuIHkCiMWaHYx3rKcCIM+w/FvjK8bgdsNAYk2eMyQTWA5d5MNYKKVXee99qyDmmzUtKqSrBkwmiCbC32PMkx7ZSRKQ50AJwNOCzHhgqIjVFJAoYBDR18r7xIrJKRFalpKS4NfizcV7eey6IH7QY6NVYlFLKEzyZIJy1sZgy9h0DTDXG5AMYY34DZgBLsa4qlgF5pQ5mzHvGmDhjTFy9evVKvuxRZZb3btwNatb1aixKKeUJnkwQSZz+rT8aSC5j3zEUNS8BYIx51hjTxRhzCVaySfBIlBVUqrx31lGriUmbl5RSVYQnE8RKoLWItBCRGlhJYHrJnUSkDVAH6yqhcJu/iEQ6HncCOgG/eTDWc1aqvPeOBWAKtHqrUqrK8FiCMMbkAfcAs7DWj/jGGLNJRJ4WkeHFdh0LTDHGFG9+CgQWiUg88B5wveN4PqGwvPfps6fnQVA4NImzLzCllHIjjw1zBTDGzMDqSyi+7ckSzyc6eV821kgmn1RY3vtU/4MxkDgPWg4Af4/+SpVSymt0JnUFlCrvfXgbpCdp85JSqkrRBHGOnJb3PrV6nHZQK6WqDk0Q56jM8t6RsVCnuX2BKaWUm2mCOEelynvnZsOuxdq8pJSqcjRBnKNS5b33Loe8LG1eUkpVOZogzoHT8t6Jc8EvEGL62RucUkq5mSaIc+C8vPc8aNYLgmrbF5hSSnmAJohzUKq89/EDcHCj9j8opaokTRDnoFR57+3zrXutv6SUqoI0QZRTmeW9a0ZBg472BaaUUh6iCaKcSpX3LiiwriBaXQR++mtUSlU9+slWTqXKex/YACcOa/OSUqrK0gRRTqXKe2+fa923HGRfUEop5UGaIMrBeXnv+VbfQ2gD+wJTSikP0gRRDqXKe+dkwJ7lEKvDW5VSVZcmiHIoVd571yIoyNX5D0qpKk0TxFmUWd47sCY0621vcEop5UGaIM6izPLeMf0gIMi+wJRSysM0QZxFqfLeR3fBke3avKSUqvI0QZxFqfLeunqcUqqa0ARxBmWW9w5vClGt7Q1OKaU8TBPEGZQq752fCzt/h1aDoDBhKKVUFaUJ4gxKlffetxpy0rV5SSlVLWiCOINS5b0T54L4QcuB9gamlFJeoAmiDGWW924SByF17AtMKaW8RBNEGUqV9z5xBPat0eGtSqlqQxNEGUqV996xADBa3lspVW1ogiiD0/LeweHQuJu9gSmllJdognCiVHlvY6zy3i0Ggn+AvcEppZSXaIJwolR575StkL5Pm5eUUtWKJggnSpX3Llw9TjuolVLViCaIEsos7x3ZGiKa2RucUkp5kSaIEk6V9y7sf8jNhl1LtHlJKVXtaIIo4VR578L+hz1LIS9Ly2sopaodTRAlOC3v7V8DYvraG5hSSnmZJohinJf3ngfNekGNWvYGp5RSXqYJophS5b3T98OhTdq8pJSqljRBFFOqvPeO+da9Dm9VSlVDmiCKcVreu1Z9aNDB3sCUUsoGHk0QInKZiGwVkUQReczJ66+KyDrHbZuIpBV77QUR2SQim0XkDRHPLuFWqrx3QYF1BdHqIvDTPKqUqn48VlhIRPyBScAlQBKwUkSmG2PiC/cxxjxQbP97ga6Ox32AvkAnx8uLgYHAAk/FW6q894H1cCJVm5eUUtWWJ78a9wASjTE7jDEngSnAiDPsPxb4yvHYAMFADSAICAQOejDW0uW9E7W8hlKqevNkgmgC7C32PMmxrRQRaQ60AOYBGGOWAfOB/Y7bLGPMZg/G6qS89zxo2Alq1/PkaZVSymd5MkE46zMwZew7BphqjMkHEJFYoC0QjZVULhKRAaVOIDJeRFaJyKqUlJQKB1qqvHfOcdi7Qq8elFLVmicTRBLQtNjzaCC5jH3HUNS8BDAKWG6MyTDGZAC/Ar1KvskY854xJs4YE1evXsW+6f+wdh8Xv7IQgE+X7uaHtftg5yIoyNP6S0qpas2TCWIl0FpEWohIDawkML3kTiLSBqgDLCu2eQ8wUEQCRCQQq4Pa7U1MP6zdx+Pf/8nRE7kApGTk8Pj3f7Jj+XQIrAVNe7r7lEopVWl4LEEYY/KAe4BZWB/u3xhjNonI0yIyvNiuY4EpxpjizU9Tge3An8B6YL0x5id3x/jirK1k5eafti0rN5/A3Qsgph8EBLn7lEopVWl4dP1MY8wMYEaJbU+WeD7Ryfvygb96MjaA5LSsUtuaykGamv0Q+4CTdyilVPVRrWeANY4IKbVtoN8G64HWX1JKVXPVOkE8MqQNIYH+p227MGAjJ0IaQ2Qrm6JSSinf4NEmJl83sqs1LePFWVtJTsuiaXggA/PjCWx3DXi2sodSSvm8ap0gwEoShYmC3Uvh40xtXlJKKap5E1Mp2+eB+EOLUnPylFKq2tEEUVziXIiOg5AIuyNRSinbaYIolJkKyWu1eUkppRw0QRTauQAwWn9JKaUcNEEUSpwHwRHQpJvdkSillE/QBAFgDGyfCy0vBD//s+2tlFLVgiYIgJQtcHy/Ni8ppVQxmiCgaPU4Le+tlFKnaIIAq3kJIDza3jiUUsqHaILIzbJmUCullDpN9U4Q85+DZxtCXrb1fGK4dZv/nL1xKaWUD6jetZgGPW7dwJEcjtkbj1JK+ZDqfQWhlFKqTJogCg18zO4IlFLKp2iCKFTY1KSUUgrQBKGUUqoMmiCUUko5pQlCKaWUU5oglFJKOaUJQimllFNijLE7BrcQkRRgtwuHiAIOuymcyhwDaBwlaRyn84U4fCEGqBpxNDfG1HP2QpVJEK4SkVXGmLjqHoPGoXFUhjh8IYbqEIc2MSmllHJKE4RSSimnNEEUec/uAPCNGEDjKEnjOJ0vxOELMUAVj0P7IJRSSjmlVxBKKaWc0gShlFLKqWqfIETkIxE5JCIbbYyhqYjMF5HNIrJJRO63KY5gEflDRNY74njKjjgcsfiLyFoR+dmuGBxx7BKRP0VknYissimGCBGZKiJbHH8jvW2IoY3jd1B4SxeRv3k7DkcsDzj+PjeKyFciEmxTHPc7Ytjkzd+Fs88sEakrIrNFJMFxX8cd56r2CQL4BLjM5hjygIeMMW2BXsAEEWlnQxw5wEXGmM5AF+AyEellQxwA9wObbTp3SYOMMV1sHO/+OjDTGHM+0Bkbfi/GmK2O30EX4ALgBDDN23GISBPgPiDOGNMB8AfG2BBHB+AOoAfWv8kwEWntpdN/QunPrMeAucaY1sBcx3OXVfsEYYz5HThicwz7jTFrHI+PY30ANLEhDmOMyXA8DXTcvD6KQUSigSuAD7x9bl8jImHAAOBDAGPMSWNMmr1RMRjYboxxpXKBKwKAEBEJAGoCyTbE0BZYbow5YYzJAxYCo7xx4jI+s0YAnzoefwqMdMe5qn2C8DUiEgN0BVbYdH5/EVkHHAJmG2PsiOM14O9AgQ3nLskAv4nIahEZb8P5WwIpwMeOJrcPRKSWDXEUNwb4yo4TG2P2AS8Be4D9wDFjzG82hLIRGCAikSJSE7gcaGpDHIUaGGP2g/WFE6jvjoNqgvAhIlIb+A74mzEm3Y4YjDH5jmaEaKCH41Laa0RkGHDIGLPam+c9g77GmG7AUKymvwFePn8A0A14xxjTFcjETc0HFSEiNYDhwLc2nb8O1rflFkBjoJaIXO/tOIwxm4H/ArOBmcB6rKbiKkUThI8QkUCs5DDZGPO93fE4mjEW4P3+mb7AcBHZBUwBLhKRL7wcwynGmGTH/SGsNvceXg4hCUgqdiU3FSth2GUosMYYc9Cm818M7DTGpBhjcoHvgT52BGKM+dAY080YMwCrySfBjjgcDopIIwDH/SF3HFQThA8QEcFqY95sjHnFxjjqiUiE43EI1n/GLd6MwRjzuDEm2hgTg9WUMc8Y4/VviAAiUktEQgsfA5diNS14jTHmALBXRNo4Ng0G4r0ZQwljsal5yWEP0EtEajr+3wzGpsEMIlLfcd8MGI29v5fpwE2OxzcBP7rjoAHuOEhlJiJfARcCUSKSBPzLGPOhl8PoC9wA/Olo/wf4hzFmhpfjaAR8KiL+WF8evjHG2DrM1GYNgGnW5xABwJfGmJk2xHEvMNnRvLMDuMWGGHC0tV8C/NWO8wMYY1aIyFRgDVaTzlrsK3fxnYhEArnABGPMUW+c1NlnFvA88I2I3IaVRK9xy7m01IZSSilntIlJKaWUU5oglFJKOaUJQimllFOaIJRSSjmlCUIppZRTmiCUOgcikl+iqqnbZjWLSIydVYWVKqnaz4NQ6hxlOUqRKFXl6RWEUm7gWDfiv471NP4QkVjH9uYiMldENjjumzm2NxCRaY61N9aLSGG5CH8Red+xxsBvjhntStlCE4RS5yakRBPTdcVeSzfG9ADewqpIi+PxZ8aYTsBk4A3H9jeAhY61N7oBmxzbWwOTjDHtgTTgKg//PEqVSWdSK3UORCTDGFPbyfZdWIst7XAUXjxgjIkUkcNAI2NMrmP7fmNMlIikANHGmJxix4jBKrHe2vH8USDQGPOM538ypUrTKwil3MeU8bisfZzJKfY4H+0nVDbSBKGU+1xX7H6Z4/FSipbEHAcsdjyeC9wFpxZpCvNWkEqVl347UerchBSruAvWWtGFQ12DRGQF1hevsY5t9wEficgjWCvDFVZivR94z1F9Mx8rWez3ePRKnQPtg1DKDRx9EHHGmMN2x6KUu2gTk1JKKaf0CkIppZRTegWhlFLKKU0QSimlnNIEoZRSyilNEEoppZzSBKGUUsqp/wfNdJ6j581iNwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from UDA_pytorch_utils import UDA_LSTMforSequential, UDA_pytorch_classifier_fit, \\\n",
    "    UDA_plot_train_val_accuracy_vs_epoch, UDA_pytorch_classifier_predict, \\\n",
    "    UDA_compute_accuracy\n",
    "\n",
    "simple_lstm_model = nn.Sequential(nn.Embedding.from_pretrained(embedding_weights),\n",
    "                                  UDA_LSTMforSequential(100, 64),\n",
    "                                  nn.Linear(64, 2))\n",
    "\n",
    "num_epochs = 10  # during optimization, how many times we look at training data\n",
    "batch_size = 512  # during optimization, how many training data to use at each step\n",
    "learning_rate = 0.01  # during optimization, how much we nudge our solution at each step\n",
    "\n",
    "train_accuracies, val_accuracies = \\\n",
    "    UDA_pytorch_classifier_fit(simple_lstm_model,\n",
    "                               torch.optim.AdamW(simple_lstm_model.parameters(),\n",
    "                                                 lr=learning_rate),\n",
    "                               nn.CrossEntropyLoss(),  # includes softmax\n",
    "                               proper_train_dataset_encoded, val_dataset_encoded,\n",
    "                               num_epochs, batch_size,\n",
    "                               sequence=True)\n",
    "\n",
    "UDA_plot_train_val_accuracy_vs_epoch(train_accuracies, val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.6325301204819277\n"
     ]
    }
   ],
   "source": [
    "test_tweets, test_labels = load_csv('HW3-data/test.csv')\n",
    "test_dataset = list(zip(test_tweets, test_labels))\n",
    "test_encoded = encode_tweets([tweet for tweet, label in test_dataset])\n",
    "test_labels = torch.tensor([label for tweet, label in test_dataset], dtype=torch.long)\n",
    "\n",
    "predicted_test_labels = UDA_pytorch_classifier_predict(simple_lstm_model,\n",
    "                                                       test_encoded,\n",
    "                                                       sequence=True)\n",
    "\n",
    "print('Test accuracy:', UDA_compute_accuracy(predicted_test_labels, test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(c) [10 points]** How well does an LSTM compare with using a classical classifier? To investigate this question, let's try training a random forest for sentiment analysis. Your code from part **(b)** should involve coming up with encoded representations of text, i.e., representing a tweet using a sequence of word indices. Use this to come up with a term frequency representation for each tweet. Feel free to remove stop words and/or apply TF-IDF weighting, and then feed the resulting feature vector representations for tweets to a random forest classifier. You can use cross-validation to select hyperparameters. Try to make the random forest classifier perform as well as possible (it is good practice in real-world applications to try to make baselines as good as possible rather than intentionally using a baseline with lousy hyperparameter choices; in particular, it is *bad* practice to tune hyperparameters carefully only on a method you're proposing while not tuning hyperparameters on baselines!). What test set accuracy are you able to achieve with the random forest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def bag_of_words_encode_tweet(args):\n",
    "    tweet, vocab_size = args\n",
    "    feature_vector = np.zeros(vocab_size)\n",
    "    for word_idx in tweet:\n",
    "        feature_vector[word_idx] += 1\n",
    "    return feature_vector\n",
    "\n",
    "def bag_of_words_encode(already_encoded_tweets):\n",
    "    p = Pool()\n",
    "    feature_vectors = p.map(bag_of_words_encode_tweet,\n",
    "                            [(encoded.cpu().numpy(), vocab_size)\n",
    "                             for encoded in already_encoded_tweets])\n",
    "    p.close()\n",
    "    return feature_vectors\n",
    "\n",
    "baseline_proper_train_encoded = bag_of_words_encode(proper_train_encoded)\n",
    "baseline_proper_train_labels = [label for tweet, label in proper_train_dataset]\n",
    "baseline_val_encoded = bag_of_words_encode(val_encoded)\n",
    "baseline_val_labels = [label for tweet, label in val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csc_matrix\n",
    "\n",
    "baseline_proper_train_encoded_sparse = csc_matrix(baseline_proper_train_encoded)\n",
    "baseline_val_encoded_sparse = csc_matrix(baseline_val_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  4.0min finished\n",
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    1.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 1) : 0.778803125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  5.0min finished\n",
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    1.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22, 1) : 0.77644375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  6.6min finished\n",
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    1.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44, 1) : 0.7723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed:  9.8min finished\n",
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(89, 1) : 0.76830625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 36 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done 100 out of 100 | elapsed: 15.7min finished\n",
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 1) : 0.7654125\n",
      "Best hyperparameter setting: (11, 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.8s finished\n"
     ]
    }
   ],
   "source": [
    "num_features = vocab_size\n",
    "hyperparameter_settings = [(max_features, min_samples_leaf)\n",
    "                           for max_features in [int(np.sqrt(num_features) / 4),\n",
    "                                                int(np.sqrt(num_features) / 2),\n",
    "                                                int(np.sqrt(num_features)),\n",
    "                                                int(np.sqrt(num_features) * 2),\n",
    "                                                int(np.sqrt(num_features) * 4)]\n",
    "                           for min_samples_leaf in [1]]\n",
    "best_val_score = -np.inf  # assumes that a higher score is better\n",
    "best_model = None\n",
    "\n",
    "for hyperparam_setting in hyperparameter_settings:\n",
    "    # your code to train and score the training data here\n",
    "    max_features, min_samples_leaf = hyperparam_setting\n",
    "    \n",
    "    clf = RandomForestClassifier(max_features=max_features,\n",
    "                                 min_samples_leaf=min_samples_leaf,\n",
    "                                 random_state=0, verbose=1, n_jobs=-1,\n",
    "                                 max_samples=0.1)\n",
    "    clf.fit(baseline_proper_train_encoded_sparse,\n",
    "            baseline_proper_train_labels)\n",
    "    \n",
    "    predicted_val_labels = clf.predict(baseline_val_encoded_sparse)\n",
    "    val_score = np.mean(baseline_val_labels == predicted_val_labels)\n",
    "    \n",
    "    print(hyperparam_setting, ':', val_score)\n",
    "    if val_score > best_val_score:  # assumes that a higher score is better\n",
    "        best_val_score = val_score\n",
    "        best_hyperparam_setting = hyperparam_setting\n",
    "        best_model = clf\n",
    "\n",
    "print('Best hyperparameter setting:', best_hyperparam_setting)\n",
    "# best_max_features, best_min_samples_leaf = best_hyperparam_setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Using backend ThreadingBackend with 36 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 0.606425702811245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=36)]: Done 100 out of 100 | elapsed:    0.2s finished\n"
     ]
    }
   ],
   "source": [
    "baseline_test_encoded_sparse = csc_matrix(bag_of_words_encode(test_encoded))\n",
    "\n",
    "baseline_predicted_test_labels = best_model.predict(baseline_test_encoded_sparse)\n",
    "print('Test accuracy:', np.mean(baseline_predicted_test_labels == test_labels.cpu().numpy()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
